# HTTP server port (1-65535)
PORT=3000

# Fastify/Pino log level (trace|debug|info|warn|error|fatal)
LOG_LEVEL=info

# Optional GitHub PAT for authenticated API calls
GITHUB_TOKEN=

# Local SQLite storage file (data/ used by default)
STORAGE_PATH=./data/cache.db

# LLM provider identifier (default Ollama integration)
LLM_PROVIDER=ollama

# Ollama base URL when using Ollama provider
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to target
OLLAMA_MODEL=llama3.1:8b

# LLM sampling temperature (0..2)
LLM_TEMPERATURE=0.2

# Maximum tokens allowed in LLM output
LLM_MAX_OUTPUT_TOKENS=900

# Maximum context tokens budget for prompts
CONTEXT_MAX_TOKENS=8192

# Maximum characters for prompts before truncation
PROMPT_MAX_CHARS=8000

# Maximum number of issues the analyze endpoint will accept
ANALYZE_MAX_ISSUES=200

# Maximum allowed characters from an issue body
ISSUE_BODY_MAX_CHARS=4000
